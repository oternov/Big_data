{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/10/23 02:45:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json,col\n",
    "from pyspark.sql.types import *\n",
    "from os.path import abspath\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.1-1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt install wget -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(path=\"AB_NYC_2019.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='2539', name='Clean & quiet apt home by the park', host_id='2787', host_name='John', neighbourhood_group='Brooklyn', neighbourhood='Kensington', latitude='40.64749', longitude='-73.97237', room_type='Private room', price='149', minimum_nights='1', number_of_reviews='9', last_review='2018-10-19', reviews_per_month='0.21', calculated_host_listings_count='6', availability_365='365')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Вывод структуры DataFrame и типов столбцов\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------+\n",
      "|        mean_price|   variance_price|total_count|\n",
      "+------------------+-----------------+-----------+\n",
      "|152.22296299343384|56902.04073527261|      48894|\n",
      "+------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание и заполнение таблицы в Hive\n",
    "data.write.mode(\"overwrite\").saveAsTable(\"AB_NYC_price\")\n",
    "\n",
    "# SQL-запрос для расчета среднего значения и дисперсии\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    AVG(price) as mean_price,\n",
    "    VARIANCE(price) as variance_price,\n",
    "    COUNT(*) as total_count\n",
    "FROM AB_NYC_price\n",
    "WHERE price IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# Выполнение запроса\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Вывод результатов\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|count_prices|total_sum|\n",
      "+------------+---------+\n",
      "|       48885|7441872.0|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Фильтрация строк, где price не является числом\n",
    "data = data.filter(F.col(\"price\").rlike(\"^[0-9]*\\\\.?[0-9]*$\"))\n",
    "data = data.fillna({\"price\": 0})  # Заменить null на 0\n",
    "\n",
    "# Преобразование столбца \"price\" из String в Float\n",
    "data = data.withColumn(\"price\", F.col(\"price\").cast(FloatType()))\n",
    "\n",
    "# Проверка на наличие null значений после преобразования\n",
    "null_count = data.filter(data.price.isNull()).count()\n",
    "if null_count > 0:\n",
    "    print(f\"В столбце 'price' {null_count} значений не удалось преобразовать в тип Float.\")\n",
    "\n",
    "# Вычисление количества значений и общей суммы\n",
    "result = data.agg(\n",
    "    F.count(\"price\").alias(\"count_prices\"),  # Подсчет количества значений в столбце 'price'\n",
    "    F.sum(\"price\").alias(\"total_sum\")        # Подсчет общей суммы в столбце 'price'\n",
    ")\n",
    "\n",
    "# Показать результат\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|     average_price|   variance_price|\n",
      "+------------------+-----------------+\n",
      "|152.23221847192391|56902.27481089729|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Вычисление среднего значения и дисперсии\n",
    "result = data.agg(\n",
    "    F.avg(\"price\").alias(\"average_price\"),\n",
    "    F.variance(\"price\").alias(\"variance_price\")\n",
    ")\n",
    "\n",
    "# Показать результат\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[33mWARNING: You are using pip version 21.2.2; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение: 152.72068718682823\n",
      "Дисперсия: 57672.845698433375\n",
      "Количество значений после очистки: 48895\n",
      "Общая сумма после очистки: 7467278.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Чтение CSV файла в DataFrame\n",
    "data = pd.read_csv(\"AB_NYC_2019.csv\")\n",
    "\n",
    "# Удаление нечисловых символов и преобразование в float\n",
    "data['price'] = data['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Проверка на наличие NaN значений\n",
    "nan_count = data['price'].isnull().sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"В столбце 'price' {nan_count} значений не удалось преобразовать в float.\")\n",
    "\n",
    "# Функция mapper для извлечения цен\n",
    "def mapper(row):\n",
    "    return row['price'] if pd.notnull(row['price']) else None\n",
    "\n",
    "# Применяем mapper к каждой строке DataFrame\n",
    "scores = data.apply(mapper, axis=1).dropna().tolist()  # Убираем NaN значения\n",
    "\n",
    "# Функция reducer для вычисления n, mean и M2\n",
    "def reducer(score_data, score_data2):\n",
    "    n, mean, M2 = score_data  # score_data - это (n, mean, M2)\n",
    "    score = score_data2  # score_data2 - это одно значение\n",
    "    n += 1\n",
    "    delta = score - mean\n",
    "    mean += delta / n\n",
    "    M2 += delta * (score - mean)\n",
    "    return n, mean, M2\n",
    "\n",
    "# Проверка наличия оценок и выполнение редукции\n",
    "if not scores:\n",
    "    print(\"Нет действительных оценок.\")\n",
    "else:\n",
    "    initial_value = (0, 0.0, 0)  # Начальное значение для n, mean, M2\n",
    "    n, mean, M2 = reduce(reducer, scores, initial_value)\n",
    "    \n",
    "    # Дисперсия\n",
    "    variance = M2 / n if n > 1 else 0  # Избегаем деления на 0\n",
    "    print(\"Среднее значение:\", mean)\n",
    "    print(\"Дисперсия:\", variance)\n",
    "\n",
    "# Подсчет значений после очистки\n",
    "cleaned_scores = data['price'].dropna().tolist()\n",
    "print(\"Количество значений после очистки:\", len(cleaned_scores))\n",
    "print(\"Общая сумма после очистки:\", sum(cleaned_scores))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Вывод:\n",
    "- Обнаружено небольшое расхождение в количестве обработанных записей (10 записей), которое отражается на всех остальных показателях.\n",
    "- Причины различий могут быть в разных подходах к фильтрации невалидных данных, различных способах обработки пропущенных значений, разных критериях отбора записей.\n",
    "- Несмотря на небольшие различия, оба метода показывают согласованные результаты: все отклонения менее 1.5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
